#!/usr/bin/env python3
"""
Example script demonstrating Gemini integration for token savings in Vexa.

This script shows how to:
1. Use the AI Service Adapter for text processing
2. Compare costs between different providers
3. Generate meeting summaries using Gemini
4. Analyze speakers with cost optimization

Usage:
    python examples/gemini_integration_example.py
"""

import asyncio
import os
import sys
import httpx
from typing import Dict, Any

# Add the ai-service-adapter to Python path for testing
sys.path.append('/root/vexa/services/ai-service-adapter')

from src.router import AIServiceRouter

async def test_ai_service_adapter():
    """Test the AI Service Adapter directly"""
    
    print("ğŸ¤– Testing AI Service Adapter")
    print("=" * 50)
    
    # Initialize the router
    router = AIServiceRouter()
    
    # Sample meeting transcript
    sample_transcript = """
    John: Good morning everyone, thanks for joining today's project review meeting.
    Sarah: Hi John, glad to be here. I have the Q3 metrics ready to share.
    Mike: Morning! I've been working on the new API endpoints we discussed last week.
    Sarah: Great! The user engagement is up 23% compared to Q2, which exceeds our target.
    John: That's fantastic news Sarah. Mike, how's the API development progressing?
    Mike: We've completed 7 out of 10 endpoints. The authentication and user management APIs are fully tested.
    John: Excellent progress. Any blockers we should address?
    Mike: Just need final approval on the rate limiting configuration from the security team.
    Sarah: I can help coordinate that. Let's schedule a follow-up next Tuesday.
    John: Perfect. Let's wrap up with action items: Sarah will coordinate security review, Mike continues API work.
    """
    
    print(f"ğŸ“ Sample transcript ({len(sample_transcript)} characters)")
    print()
    
    try:
        # Test 1: Generate meeting summary
        print("1ï¸âƒ£ Testing Meeting Summary Generation")
        print("-" * 30)
        
        summary_response = await router.summarize(sample_transcript, max_length=100)
        
        print(f"âœ… Summary generated using {summary_response.provider} ({summary_response.model})")
        print(f"ğŸ’° Cost: ${summary_response.token_usage.cost_usd:.6f}")
        print(f"ğŸ¯ Tokens: {summary_response.token_usage.input_tokens} in â†’ {summary_response.token_usage.output_tokens} out")
        print(f"ğŸ“„ Summary:\n{summary_response.content}")
        print()
        
        # Test 2: Analyze speakers
        print("2ï¸âƒ£ Testing Speaker Analysis")
        print("-" * 30)
        
        speaker_response = await router.analyze_speakers(sample_transcript)
        
        print(f"âœ… Analysis completed using {speaker_response.provider} ({speaker_response.model})")
        print(f"ğŸ’° Cost: ${speaker_response.token_usage.cost_usd:.6f}")
        print(f"ğŸ¯ Tokens: {speaker_response.token_usage.input_tokens} in â†’ {speaker_response.token_usage.output_tokens} out")
        print(f"ğŸ‘¥ Analysis:\n{speaker_response.content}")
        print()
        
        # Test 3: Generate custom analysis
        print("3ï¸âƒ£ Testing Custom Text Generation")
        print("-" * 30)
        
        custom_prompt = f"""Based on this meeting transcript, identify:
1. Key decisions made
2. Action items assigned
3. Next steps mentioned

Transcript:
{sample_transcript}

Analysis:"""
        
        custom_response = await router.generate_text(custom_prompt, max_tokens=300)
        
        print(f"âœ… Analysis generated using {custom_response.provider} ({custom_response.model})")
        print(f"ğŸ’° Cost: ${custom_response.token_usage.cost_usd:.6f}")
        print(f"ğŸ¯ Tokens: {custom_response.token_usage.input_tokens} in â†’ {custom_response.token_usage.output_tokens} out")
        print(f"ğŸ“Š Analysis:\n{custom_response.content}")
        print()
        
        # Display usage statistics
        print("4ï¸âƒ£ Usage Statistics")
        print("-" * 30)
        
        stats = router.get_usage_stats()
        total_cost = (
            summary_response.token_usage.cost_usd + 
            speaker_response.token_usage.cost_usd + 
            custom_response.token_usage.cost_usd
        )
        
        print(f"ğŸ’³ Total session cost: ${total_cost:.6f}")
        print(f"ğŸ“ˆ Daily usage: {stats['daily_usage']}")
        print(f"ğŸ”§ Available providers: {stats['available_providers']}")
        print(f"âš™ï¸  Cost optimization: {'Enabled' if stats['config']['cost_optimization_enabled'] else 'Disabled'}")
        print()
        
        # Cost comparison insight
        print("5ï¸âƒ£ Cost Savings Analysis")
        print("-" * 30)
        
        # Estimate what this would cost with a more expensive model
        estimated_openai_cost = total_cost * 10  # Rough estimation
        savings_percentage = ((estimated_openai_cost - total_cost) / estimated_openai_cost) * 100
        
        print(f"ğŸ¯ Gemini cost: ${total_cost:.6f}")
        print(f"ğŸ’¸ Estimated OpenAI GPT-4 cost: ${estimated_openai_cost:.6f}")
        print(f"ğŸ’° Potential savings: {savings_percentage:.1f}%")
        
    except Exception as e:
        print(f"âŒ Error during testing: {e}")
        print("Make sure you have GEMINI_API_KEY set in your environment")

async def test_api_endpoints():
    """Test the AI Service Adapter API endpoints"""
    
    print("\nğŸŒ Testing API Endpoints")
    print("=" * 50)
    
    base_url = "http://localhost:8000"
    
    # Sample data
    sample_text = "This is a test meeting about project planning and resource allocation."
    
    async with httpx.AsyncClient() as client:
        
        try:
            # Test health endpoint
            print("ğŸ¥ Testing health endpoint...")
            health_response = await client.get(f"{base_url}/health")
            if health_response.status_code == 200:
                print(f"âœ… Service healthy: {health_response.json()}")
            else:
                print(f"âŒ Health check failed: {health_response.status_code}")
            
            # Test summarization endpoint
            print("\nğŸ“ Testing summarization API...")
            summary_data = {"text": sample_text, "max_length": 50}
            summary_response = await client.post(f"{base_url}/summarize", json=summary_data)
            
            if summary_response.status_code == 200:
                result = summary_response.json()
                print(f"âœ… Summary: {result['summary']}")
                print(f"ğŸ’° Cost: ${result['token_usage']['cost_usd']:.6f}")
            else:
                print(f"âŒ Summarization failed: {summary_response.status_code}")
            
            # Test usage endpoint
            print("\nğŸ“Š Testing usage statistics...")
            usage_response = await client.get(f"{base_url}/usage")
            if usage_response.status_code == 200:
                usage_stats = usage_response.json()
                print(f"âœ… Usage stats: {usage_stats}")
            else:
                print(f"âŒ Usage stats failed: {usage_response.status_code}")
                
        except httpx.ConnectError:
            print("âŒ Could not connect to AI Service Adapter")
            print("ğŸ’¡ Make sure to start the service first:")
            print("   cd /root/vexa/services/ai-service-adapter")
            print("   python -m src.main")

async def main():
    """Main demo function"""
    
    print("ğŸš€ Vexa Gemini Integration Demo")
    print("=" * 60)
    print()
    
    # Check environment
    gemini_key = os.getenv("GEMINI_API_KEY")
    if not gemini_key or gemini_key == "your-gemini-api-key-here":
        print("âš ï¸  Warning: GEMINI_API_KEY not properly configured")
        print("ğŸ’¡ Set your Gemini API key in .env file:")
        print("   GEMINI_API_KEY=your-actual-api-key")
        print()
    
    # Test the service adapter directly
    await test_ai_service_adapter()
    
    # Test API endpoints (optional - requires service to be running)
    await test_api_endpoints()
    
    print("\nğŸ‰ Demo completed!")
    print("\nğŸ“‹ Next Steps:")
    print("1. Set your GEMINI_API_KEY in the .env file")
    print("2. Start the AI Service Adapter: docker-compose up ai-service-adapter")
    print("3. Integrate with your existing services using the API endpoints")
    print("4. Monitor usage and costs through the /usage endpoint")

if __name__ == "__main__":
    asyncio.run(main())